기간: 2023/3/22 ~ 2023/3/31

주제: 독립연구 준비 - 주제를 정하기 위해 Word2Vec에 대해 알아보기

내용: Word2vec 

1. Word2vec 
지난 인공지능 수업 내용 중 자연어 처리 부분을 간단히 복습

- BERT, GTP
- Word2vec의 두 가지 학습 방법 중 CBoW (Continuous Bag of Words)
Distributional Hypothesis 분포 가설: 단어의 의미는 주변 단어(맥락)에 의해 형성된다 
= 의미가 비슷하면 문장에서 위치하는 곳도 비슷하다


2. Korean Word2Vec (https://word2vec.kr/search/?query)
한국어 말뭉치로 구현된 Word2Vec 알고리즘으로 만든 웹 사이트를 사용

- 식을 입력하고 출력된 결과를 보면서 벡터 공간에 단어들이 어떤 식으로 분포해있을지 추측
- 어떤 부분이 일상 언어를 사용할 때의 직관에서 벗어나 있는지 생각

- 관련 논문
Word2vec 모델로 학습된 단어 벡터의 의미 관계 분석
하지만 word2vec 모델에도 한계가 존재한다. 
예를 들어, ‘소년’과 ‘소녀’라는 단어는 의미에서 명백히 반대되는 말이지만, 비슷한 문맥에서 발생할 가능성이 높다. 
따라서 벡터 공간에서 이 두 단어의 벡터는 가까이 임베딩될 가능성도 역시 높다. 
보통 학습 말뭉치가 충분히 크다면, 이 두 단어 벡터의 거리가 충분히 멀리 임베딩 될 수 있다. 
그러나 일반적으로 word2vec 모델에 의해 가까운 위치에 임베딩된 단어들이 항상 의미론적으로 유사하다고 볼 수는 없는 한계가 존재한다.
(강형석, 양장훈, 2019, pp. 1089)


3. 영어 Word2vec 실습
이 자료를 참고하여 colab에서 실습 (https://wikidocs.net/50739)

- 파이썬의 gensim 패키지에서 Word2Vec을 제공
- 단어를 임베딩 벡터로 변환시킨 후 단어들 간 유사도를 확인
